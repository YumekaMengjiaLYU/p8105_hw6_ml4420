---
title: "p8105_hw6_ml4420"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(mgcv)
library(corrplot)
knitr::opts_chunk$set(echo = TRUE)
```


## Problem 1
```{r}
birthwt_data = read.csv("data/birthweight.csv") %>%
  janitor::clean_names() %>%
  na.omit() %>%
  mutate(babysex = recode_factor(babysex, `1` = "male", `2` = "female")) %>%
  mutate(frace = recode_factor(frace, `1` = "White", `2` = "Black", `3` = "Asian", 
                               `4` = "Puerto Rican", `8` = "Other", `9` = "Unknown")) %>%
  mutate(malform = recode_factor(malform, `0` = "absent", `1` = "present")) %>%
  mutate(mrace = recode_factor(mrace, `1` = "White", `2` = "Black", `3` = "Asian",
                               `4` = "Puerto Rican", `8` = "Other")) 

# fit a regression using all predictors
linear_mod = lm( bwt ~ ., data = birthwt_data) 
summary(linear_mod)


# carry out backward elimination : take out non-significant values one at a time

```

In the regression summary, we can see that there are three variables with NAs. To further investigate, we choose to do a 

```{r}

v
```

We find out that `pnumlbw` and `pnumgsa` have zero variance and thus are not variables. Including them in the regression model would not help us better predict the outcome as they do not carry information.

```{r}
birthwt_data1 = birthwt_data %>%
  select(-pnumlbw, -pnumsga)

var(birthwt_data$wtgain)
linear_mod1 = lm( bwt ~ ., data = birthwt_data1) 
summary(linear_mod1)



```

For `wtgain`, we still get NAs. It indicates that our predictor variables are not all linearly independent. We can choose to exclude the variable and the model will still have the same coefficients as before. Therefore, we take out `wtgain`.

```{r}
birthwt_data2 = birthwt_data1%>%
  select(-wtgain)

linear_mod2 = lm( bwt ~ ., data = birthwt_data2) 
summary(linear_mod2)

```

With all the variables having coefficients, we can now carry out backward elimination, meaning that we take out non-significant variables one at a time.

Since `fraceOther` has the highest p-value among all the covariates, we take it out. We should see whether the categorical variable as a whole is significant or not, as we cannot include some categories of a variable and exclude some categories having insignificant difference.

```{r}
test_model = lm(bwt ~ babysex + bhead + blength + bwt +
                   delwt + fincome + gaweeks + malform + 
                   menarche + mheight + momage + mrace +
                   parity + ppbmi + ppwt + smoken, data = birthwt_data2)

anova(linear_mod2, test_model)

summary(test_model)
```
Since p-value is bigger than 0.05, it means the difference is significant and the variable `frace` should **not** be included in the model.

Now the covariate with the highest p-value is `malform`. 
```{r}
test_model1 = lm(bwt ~ babysex + bhead + blength + bwt +
                   delwt + fincome + gaweeks +  
                   menarche + mheight + momage + mrace +
                   parity + ppbmi + ppwt + smoken, data = birthwt_data2)

anova(test_model, test_model1)

summary(test_model1)
```
Since the p-value is greater than 0.05, it means the difference is insignificant and the variable `malform` should not be included in the model.

Now we take out `ppbmi` which is the covariate with the largest p-value.

```{r}
linear_mod3 = lm(bwt ~ babysex + bhead + blength + bwt +
                   delwt + fincome + gaweeks +  
                   menarche + mheight + momage + mrace +
                   parity + ppwt + smoken, data = birthwt_data2)
summary(linear_mod3)

```
Now we take out `momage` which is the covariate with the largest p-value.
```{r}
linear_mod4 = lm(bwt ~ babysex + bhead + blength + bwt +
                   delwt + fincome + gaweeks +  
                   menarche + mheight + mrace +
                   parity + ppwt + smoken, data = birthwt_data2)
summary(linear_mod4)
```
Now we take out `menarche` which is the covariate with the largest p-value.

```{r}
linear_mod5 = lm(bwt ~ babysex + bhead + blength + bwt +
                   delwt + fincome + gaweeks +  
                   mheight + mrace +
                   parity + ppwt + smoken, data = birthwt_data2)
summary(linear_mod5)
```

Since `mraceAsian` has the highest p-value among all the covariates, we take it out. We should see whether the categorical variable as a whole is significant or not, as we cannot include some categories of a variable and exclude some categories having insignificant difference.

```{r}
test_model2 = lm(bwt ~ babysex + bhead + blength + bwt +
                   delwt + fincome + gaweeks +  
                   mheight + 
                   parity + ppwt + smoken, data = birthwt_data2)
anova(linear_mod5, test_model2)

summary(test_model5)
```

Since p-value is less than 0.5, it means the difference is significant and the variable `mrace` should be included in the model.

Now we take out `fincome` which is the covariate with the highest p-value

```{r}
linear_mod6 = lm(bwt ~ babysex + bhead + blength + bwt +
                   delwt + gaweeks +
                   mheight + mrace +
                   parity + ppwt + smoken, data = birthwt_data2)
summary(linear_mod6)

birthwt_data3 = birthwt_data%>%
  select(babysex,bhead,blength,bwt,
                   delwt, gaweeks,
                   mheight, mrace,
                   parity, ppwt, smoken)
smooth_mod = mgcv::gam(bwt ~ s(bhead) +s(blength)+s(babysex), data = birthwt_data3)
wiggly_mod = mgcv::gam(y ~ s(x, k = 30), sp = 10e-6, data = train_df)
```

```{r}
birthwt_data2 %>%
  modelr::add_predictions(linear_mod6) %>%
  modelr::add_residuals(linear_mod6) %>%
  ggplot(aes(x = resid,y = pred)) +
  geom_point()

# compare
reference_model1 = lm(bwt~ blength + gaweeks, data = birthwt_data)

reference_model2 = lm(bwt ~ bhead*blength*babysex, data = birthwt_data)

cv_df =
  crossv_mc(birthwt_data3, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df %>% 
  mutate(final_mod  = map(train, ~lm(bwt ~ babysex + bhead + blength + bwt +
                                      delwt + gaweeks + mheight + mrace +
                                      parity + ppwt + smoken, data = .x)),
         ref_mod1     = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         ref_mod2  = map(train, ~lm(bwt ~ bhead*blength*babysex, data = .x))) %>% 
  mutate(rmse_linear = map2_dbl(final_mod, test, ~rmse(model = .x, data = .y)),
         rmse_pwl    = map2_dbl(ref_mod1, test, ~rmse(model = .x, data = .y)),
         rmse_smooth = map2_dbl(ref_mod2, test, ~rmse(model = .x, data = .y)))

cv_df %>% 
  select(starts_with("rmse")) %>% 
pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

The model with the smallest root mean square error is the model we built from backward elimination.